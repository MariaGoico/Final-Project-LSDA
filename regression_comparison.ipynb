{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5ce3cf4",
   "metadata": {},
   "source": [
    "# Regression Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0090513c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import csv\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import shutil\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, when, month, lag\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor, GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, TrainValidationSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f8a578",
   "metadata": {},
   "source": [
    "## Auxiliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a29f3781",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_OUTPUT_DIR = \"saved_models\"\n",
    "MASTER_LOG_FILE = \"model_comparison.csv\"\n",
    "BASE_DATA_PATH = \"processed_data\"\n",
    "\n",
    "\n",
    "def get_paths(years):\n",
    "    \"\"\"Generates file paths for the specific years.\"\"\"\n",
    "    return [f\"{BASE_DATA_PATH}/climate_{y}.parquet\" for y in years]\n",
    "\n",
    "def save_training_history(model, output_dir, model_name):\n",
    "    \"\"\"\n",
    "    Extracts iteration history (Objective History) if available.\n",
    "    Works specifically for LinearRegression.\n",
    "    \"\"\"\n",
    "    history_path = os.path.join(output_dir, \"training_history.csv\")\n",
    "    \n",
    "    if hasattr(model, \"summary\") and hasattr(model.summary, \"objectiveHistory\"):\n",
    "        history = model.summary.objectiveHistory\n",
    "        \n",
    "        df_hist = pd.DataFrame({\n",
    "            \"Iteration\": range(1, len(history) + 1),\n",
    "            \"Objective_Loss\": history\n",
    "        })\n",
    "        df_hist.to_csv(history_path, index=False)\n",
    "        print(f\"   [v] Training history (Loss) saved to: {history_path}\")\n",
    "        \n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.plot(df_hist[\"Iteration\"], df_hist[\"Objective_Loss\"], marker='o')\n",
    "        plt.title(f\"Convergence Curve - {model_name}\")\n",
    "        plt.xlabel(\"Iteration\")\n",
    "        plt.ylabel(\"Loss (Objective Function)\")\n",
    "        plt.grid(True)\n",
    "        plt.savefig(os.path.join(output_dir, \"convergence_plot.png\"))\n",
    "        plt.close()\n",
    "    else:\n",
    "        print(\"   [!] This algorithm does not expose iterative history (objectiveHistory).\")\n",
    "\n",
    "def evaluate_and_log(predictions, target_col, time_taken, output_dir, model_name):\n",
    "    \"\"\"\n",
    "    Calculates metrics, saves them to the Master CSV, and saves a local JSON.\n",
    "    \"\"\"\n",
    "    evaluator = RegressionEvaluator(labelCol=target_col, predictionCol=\"prediction\")\n",
    "    \n",
    "    r2 = evaluator.setMetricName(\"r2\").evaluate(predictions)\n",
    "    rmse = evaluator.setMetricName(\"rmse\").evaluate(predictions)\n",
    "    mae = evaluator.setMetricName(\"mae\").evaluate(predictions)\n",
    "    \n",
    "    snow_subset = predictions.filter(col(target_col) > 0)\n",
    "    if snow_subset.count() > 0:\n",
    "        r2_snow = evaluator.setMetricName(\"r2\").evaluate(snow_subset)\n",
    "    else:\n",
    "        r2_snow = 0.0\n",
    "\n",
    "    metrics = {\n",
    "        \"Model\": model_name,\n",
    "        \"Time_Sec\": round(time_taken, 2),\n",
    "        \"R2_Global\": round(r2, 4),\n",
    "        \"RMSE_Global\": round(rmse, 4),\n",
    "        \"MAE_Global\": round(mae, 4),\n",
    "        \"R2_Snow_Only\": round(r2_snow, 4)\n",
    "    }\n",
    "\n",
    "    file_exists = os.path.isfile(MASTER_LOG_FILE)\n",
    "    with open(MASTER_LOG_FILE, mode='a', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=metrics.keys())\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(metrics)\n",
    "    \n",
    "    with open(os.path.join(output_dir, \"metrics.json\"), 'w') as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "\n",
    "    print(f\"\\n--- METRICS ({model_name}) ---\")\n",
    "    print(f\"R2: {r2:.4f} | RMSE: {rmse:.4f} | R2 Snow Only: {r2_snow:.4f}\")\n",
    "    return metrics\n",
    "\n",
    "def plot_predictions(predictions, target_col, output_dir, model_name):\n",
    "    \"\"\"Generates and saves Scatter and Residual plots.\"\"\"\n",
    "    print(\"   Generating plots...\")\n",
    "    \n",
    "    pdf = predictions.select(target_col, \"prediction\").sample(False, 0.05, seed=42).toPandas()\n",
    "    \n",
    "    plt.figure(figsize=(14, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.scatterplot(x=pdf[target_col], y=pdf[\"prediction\"], alpha=0.3)\n",
    "    plt.plot([pdf[target_col].min(), pdf[target_col].max()], \n",
    "             [pdf[target_col].min(), pdf[target_col].max()], 'r--', lw=2)\n",
    "    plt.xlabel('Reality (Actual Value)')\n",
    "    plt.ylabel('Prediction')\n",
    "    plt.title(f'Prediction vs Reality - {model_name}')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    residuals = pdf[target_col] - pdf[\"prediction\"]\n",
    "    sns.histplot(residuals, bins=50, kde=True)\n",
    "    plt.xlabel('Error (Real - Predicted)')\n",
    "    plt.title('Residuals Distribution')\n",
    "    \n",
    "    plt.savefig(os.path.join(output_dir, \"prediction_plots.png\"))\n",
    "    plt.close()\n",
    "    print(f\"   [v] Plots saved in: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd02204",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2596fcd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STARTING: LinearRegression_Baseline ---\n",
      "Output Directory: saved_models/LinearRegression_Baseline\n",
      "1. Loading Data (Full Dataset)...\n",
      "   Features (19): ['LATITUDE', 'LONGITUDE', 'ELEVATION', 'TEMP', 'DEWP', 'SLP', 'STP', 'VISIB', 'WDSP', 'MXSPD', 'MAX', 'MIN', 'PRCP', 'is_Fog', 'is_Rain', 'is_Snow', 'is_Hail', 'is_Thunder', 'is_Tornado']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Training Rows: 51,241,804\n",
      "2. Training LinearRegression_Baseline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Training completed in 49.48 seconds.\n",
      "3. Processing Results...\n",
      "   [v] Spark Model saved to: saved_models/LinearRegression_Baseline/spark_model\n",
      "   [v] Training history (Loss) saved to: saved_models/LinearRegression_Baseline/training_history.csv\n",
      "   Generating predictions on Test Set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- METRICS (LinearRegression_Baseline) ---\n",
      "R2: 0.3790 | RMSE: 9.0236 | R2 Snow Only: 0.0118\n",
      "   Generating plots...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   [v] Plots saved in: saved_models/LinearRegression_Baseline\n",
      "\n",
      "--- PROCESS FINISHED SUCCESSFULLY ---\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"LinearRegression_Baseline\"\n",
    "\n",
    "print(f\"--- STARTING: {MODEL_NAME} ---\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(MODEL_NAME) \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"10g\") \\\n",
    "    .config(\"spark.executor.memory\", \"10g\") \\\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"128m\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "model_output_dir = os.path.join(BASE_OUTPUT_DIR, MODEL_NAME)\n",
    "if os.path.exists(model_output_dir):\n",
    "    shutil.rmtree(model_output_dir)\n",
    "os.makedirs(model_output_dir)\n",
    "\n",
    "print(f\"Output Directory: {model_output_dir}\")\n",
    "\n",
    "train_years = range(2010, 2021) \n",
    "val_years   = range(2021, 2023) \n",
    "test_years  = range(2023, 2025) \n",
    "\n",
    "print(\"1. Loading Data (Full Dataset)...\")\n",
    "try:\n",
    "    train_df = spark.read.parquet(*get_paths(train_years))\n",
    "    val_df   = spark.read.parquet(*get_paths(val_years))\n",
    "    test_df  = spark.read.parquet(*get_paths(test_years))\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    spark.stop()\n",
    "    raise e\n",
    "\n",
    "target_col = \"SNDP\"\n",
    "ignore_cols = [target_col, \"DATE\", \"STATION\", \"NAME\", \"features\", \"prediction\", \"FRSHTT\"]\n",
    "valid_types = ['int', 'bigint', 'float', 'double', 'tinyint', 'smallint']\n",
    "\n",
    "dtypes = train_df.dtypes\n",
    "feature_cols = [c for c, t in dtypes if t in valid_types and c not in ignore_cols]\n",
    "print(f\"   Features ({len(feature_cols)}): {feature_cols}\")\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\").setHandleInvalid(\"skip\")\n",
    "\n",
    "train_vec = assembler.transform(train_df)\n",
    "val_vec   = assembler.transform(val_df)\n",
    "test_vec  = assembler.transform(test_df)\n",
    "\n",
    "train_vec.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "print(f\"   Training Rows: {train_vec.count():,}\")\n",
    "\n",
    "print(f\"2. Training {MODEL_NAME}...\")\n",
    "\n",
    "lr = LinearRegression(\n",
    "    featuresCol=\"features\", \n",
    "    labelCol=target_col,\n",
    "    maxIter=50, \n",
    "    regParam=0.1, \n",
    "    elasticNetParam=0.5\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "model = lr.fit(train_vec)\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "\n",
    "print(f\"   Training completed in {duration:.2f} seconds.\")\n",
    "\n",
    "print(\"3. Processing Results...\")\n",
    "\n",
    "model_save_path = os.path.join(model_output_dir, \"spark_model\")\n",
    "model.write().overwrite().save(model_save_path)\n",
    "print(f\"   [v] Spark Model saved to: {model_save_path}\")\n",
    "\n",
    "save_training_history(model, model_output_dir, MODEL_NAME)\n",
    "\n",
    "print(\"   Generating predictions on Test Set...\")\n",
    "test_preds = model.transform(test_vec)\n",
    "metrics = evaluate_and_log(test_preds, target_col, duration, model_output_dir, MODEL_NAME)\n",
    "\n",
    "plot_predictions(test_preds, target_col, model_output_dir, MODEL_NAME)\n",
    "\n",
    "print(\"\\n--- PROCESS FINISHED SUCCESSFULLY ---\")\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5868c6",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34b6a075",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_OUTPUT_DIR = \"saved_models\"     \n",
    "MASTER_LOG_FILE = \"model_comparison.csv\" \n",
    "BASE_DATA_PATH = \"processed_data\"    \n",
    "\n",
    "def get_paths(years):\n",
    "    return [f\"{BASE_DATA_PATH}/climate_{y}.parquet\" for y in years]\n",
    "\n",
    "def save_feature_importance(model, feature_cols, output_dir):\n",
    "    \"\"\"\n",
    "    Extracts Feature Importance from Random Forest and saves it to CSV.\n",
    "    \"\"\"\n",
    "    if hasattr(model, \"featureImportances\"):\n",
    "        importances = model.featureImportances\n",
    "        feature_list = []\n",
    "        for i, col_name in enumerate(feature_cols):\n",
    "            feature_list.append({\"Feature\": col_name, \"Importance\": float(importances[i])})\n",
    "        \n",
    "        df_imp = pd.DataFrame(feature_list).sort_values(by=\"Importance\", ascending=False)\n",
    "        csv_path = os.path.join(output_dir, \"feature_importance.csv\")\n",
    "        df_imp.to_csv(csv_path, index=False)\n",
    "        print(f\"   [v] Feature Importance saved to: {csv_path}\")\n",
    "        \n",
    "        print(\"   --- TOP 5 FEATURES ---\")\n",
    "        print(df_imp.head(5))\n",
    "    else:\n",
    "        print(\"   [!] This model does not support feature importance.\")\n",
    "\n",
    "def evaluate_and_log(predictions, target_col, time_taken, output_dir, model_name):\n",
    "    \"\"\"\n",
    "    Calculates R2, RMSE, MAE, saves to Master CSV and local JSON.\n",
    "    \"\"\"\n",
    "    evaluator = RegressionEvaluator(labelCol=target_col, predictionCol=\"prediction\")\n",
    "    \n",
    "    r2 = evaluator.setMetricName(\"r2\").evaluate(predictions)\n",
    "    rmse = evaluator.setMetricName(\"rmse\").evaluate(predictions)\n",
    "    mae = evaluator.setMetricName(\"mae\").evaluate(predictions)\n",
    "    \n",
    "    snow_subset = predictions.filter(col(target_col) > 0)\n",
    "    if snow_subset.count() > 0:\n",
    "        r2_snow = evaluator.setMetricName(\"r2\").evaluate(snow_subset)\n",
    "    else:\n",
    "        r2_snow = 0.0\n",
    "\n",
    "    metrics = {\n",
    "        \"Model\": model_name,\n",
    "        \"Time_Sec\": round(time_taken, 2),\n",
    "        \"R2_Global\": round(r2, 4),\n",
    "        \"RMSE_Global\": round(rmse, 4),\n",
    "        \"MAE_Global\": round(mae, 4),\n",
    "        \"R2_Snow_Only\": round(r2_snow, 4)\n",
    "    }\n",
    "\n",
    "    file_exists = os.path.isfile(MASTER_LOG_FILE)\n",
    "    with open(MASTER_LOG_FILE, mode='a', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=metrics.keys())\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(metrics)\n",
    "    \n",
    "    with open(os.path.join(output_dir, \"metrics.json\"), 'w') as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "\n",
    "    print(f\"\\n--- METRICS ({model_name}) ---\")\n",
    "    print(f\"R2: {r2:.4f} | RMSE: {rmse:.4f} | R2 Snow Only: {r2_snow:.4f}\")\n",
    "    return metrics\n",
    "\n",
    "def plot_predictions(predictions, target_col, output_dir, model_name):\n",
    "    \"\"\"Generates and saves Scatter and Residual plots.\"\"\"\n",
    "    print(\"   Generating plots...\")\n",
    "    \n",
    "    pdf = predictions.select(target_col, \"prediction\").sample(False, 0.05, seed=42).toPandas()\n",
    "    \n",
    "    plt.figure(figsize=(14, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.scatterplot(x=pdf[target_col], y=pdf[\"prediction\"], alpha=0.3)\n",
    "    plt.plot([pdf[target_col].min(), pdf[target_col].max()], \n",
    "             [pdf[target_col].min(), pdf[target_col].max()], 'r--', lw=2)\n",
    "    plt.xlabel('Reality (Actual Value)')\n",
    "    plt.ylabel('Prediction')\n",
    "    plt.title(f'Prediction vs Reality - {model_name}')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    residuals = pdf[target_col] - pdf[\"prediction\"]\n",
    "    sns.histplot(residuals, bins=50, kde=True)\n",
    "    plt.xlabel('Error (Real - Predicted)')\n",
    "    plt.title('Residuals Distribution')\n",
    "    \n",
    "    plt.savefig(os.path.join(output_dir, \"prediction_plots.png\"))\n",
    "    plt.close()\n",
    "    print(f\"   [v] Plots saved in: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420f79b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STARTING: RandomForest_Baseline ---\n",
      "1. Loading Data...\n",
      "   Features (19): ['LATITUDE', 'LONGITUDE', 'ELEVATION', 'TEMP', 'DEWP', 'SLP', 'STP', 'VISIB', 'WDSP', 'MXSPD', 'MAX', 'MIN', 'PRCP', 'is_Fog', 'is_Rain', 'is_Snow', 'is_Hail', 'is_Thunder', 'is_Tornado']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Training Rows: 51,241,804\n",
      "2. Training RandomForest_Baseline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Training completed in 1564.64 seconds.\n",
      "3. Processing Results...\n",
      "   [v] Feature Importance saved to: saved_models/RandomForest_Baseline/feature_importance.csv\n",
      "   --- TOP 5 FEATURES ---\n",
      "     Feature  Importance\n",
      "11       MIN    0.326477\n",
      "10       MAX    0.233745\n",
      "3       TEMP    0.129317\n",
      "0   LATITUDE    0.072881\n",
      "4       DEWP    0.067982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- METRICS (RandomForest_Baseline) ---\n",
      "R2: 0.6211 | RMSE: 7.0489 | R2 Snow Only: 0.3284\n",
      "   Generating plots...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   [v] Plots saved in: saved_models/RandomForest_Baseline\n",
      "\n",
      "--- PROCESS FINISHED ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import StorageLevel\n",
    "import pyspark.sql.functions as sql_f\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, RegressionEvaluator\n",
    "\n",
    "\"\"\"\n",
    "PySpark ML implementation for scalability experiments\n",
    "\"\"\"\n",
    "\n",
    "BASE_DATA_PATH = \"processed_data\"  # adjust to your path\n",
    "BASE_OUTPUT_DIR = \"model_outputs\"\n",
    "TARGET_COL = \"SNDP\"      # target column\n",
    "IGNORE_COLS = [TARGET_COL, \"DATE\", \"STATION\", \"NAME\", \"features\", \"prediction\", \"FRSHTT\"]\n",
    "VALID_TYPES = ['int', 'bigint', 'float', 'double', 'tinyint', 'smallint']\n",
    "\n",
    "MODEL_TYPE = \"regression\"  # \"regression\" or \"classification\"\n",
    "\n",
    "\n",
    "def get_paths(years):\n",
    "    return [f\"{BASE_DATA_PATH}/climate_{y}.parquet\" for y in years]\n",
    "\n",
    "\n",
    "def evaluate_model(predictions_df, target_col, model_type):\n",
    "    \"\"\"\n",
    "    Evaluate model predictions\n",
    "    \"\"\"\n",
    "    if model_type == \"regression\":\n",
    "        evaluator = RegressionEvaluator(\n",
    "            labelCol=target_col,\n",
    "            predictionCol=\"prediction\",\n",
    "            metricName=\"rmse\"\n",
    "        )\n",
    "        rmse = evaluator.evaluate(predictions_df)\n",
    "        return rmse\n",
    "    else:\n",
    "        evaluator = MulticlassClassificationEvaluator(\n",
    "            labelCol=target_col,\n",
    "            predictionCol=\"prediction\",\n",
    "            metricName=\"accuracy\"\n",
    "        )\n",
    "        accuracy = evaluator.evaluate(predictions_df)\n",
    "        return accuracy\n",
    "\n",
    "\n",
    "def main(argv):\n",
    "    # Parse arguments\n",
    "    cores = int(argv[0])            # number of partitions / Spark cores\n",
    "    pct = int(argv[1])              # percentage of training data to use\n",
    "    filename = argv[2] if len(argv) > 2 else \"scalability_results.csv\"\n",
    "\n",
    "    MODEL_NAME = f\"DecisionTree_cores{cores}_pct{pct}\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"--- STARTING: {MODEL_NAME} ---\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "    # Start Spark\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .appName(MODEL_NAME)\n",
    "        .master(f\"local[{cores}]\")\n",
    "        .config(\"spark.driver.memory\", \"12g\")\n",
    "        .config(\"spark.executor.memory\", \"12g\")\n",
    "        .config(\"spark.sql.files.maxPartitionBytes\", \"128m\")\n",
    "        .config(\"spark.driver.maxResultSize\", \"4g\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "    # Create output directory\n",
    "    model_output_dir = os.path.join(BASE_OUTPUT_DIR, MODEL_NAME)\n",
    "    if os.path.exists(model_output_dir):\n",
    "        shutil.rmtree(model_output_dir)\n",
    "    os.makedirs(model_output_dir)\n",
    "\n",
    "    # Define data splits\n",
    "    train_years = range(2021, 2023)\n",
    "    val_years   = range(2023, 2024)\n",
    "\n",
    "    print(\"1. Loading Data...\")\n",
    "    try:\n",
    "        df_train = spark.read.parquet(*get_paths(train_years))\n",
    "        df_val   = spark.read.parquet(*get_paths(val_years))\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        spark.stop()\n",
    "        raise e\n",
    "\n",
    "    # Select features\n",
    "    dtypes = df_train.dtypes\n",
    "    feature_cols = [c for c, t in dtypes if t in VALID_TYPES and c not in IGNORE_COLS]\n",
    "    print(f\"   Features ({len(feature_cols)}): {feature_cols[:5]}... (showing first 5)\")\n",
    "\n",
    "    # Prepare training data\n",
    "    df_train = df_train.select(feature_cols + [TARGET_COL])\n",
    "    \n",
    "    # Sample fraction if pct < 100\n",
    "    if pct < 100:\n",
    "        print(f\"   Sampling {pct}% of training data...\")\n",
    "        df_train = df_train.sample(fraction=pct/100.0, seed=42)\n",
    "\n",
    "    # Repartition training data\n",
    "    df_train = df_train.repartition(cores)\n",
    "    \n",
    "    # Assemble features\n",
    "    print(\"   Assembling features...\")\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\").setHandleInvalid(\"skip\")\n",
    "    train_vec = assembler.transform(df_train)\n",
    "    val_vec = assembler.transform(df_val)\n",
    "\n",
    "    # Cache and materialize\n",
    "    train_vec.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "    train_count = train_vec.count()\n",
    "    print(f\"   Training Rows: {train_count:,}\")\n",
    "    print(f\"   Partitions: {train_vec.rdd.getNumPartitions()}\")\n",
    "\n",
    "    # Build model\n",
    "    print(f\"\\n2. Training {MODEL_NAME}...\")\n",
    "    if MODEL_TYPE == \"regression\":\n",
    "        model = DecisionTreeRegressor(\n",
    "            featuresCol=\"features\",\n",
    "            labelCol=TARGET_COL,\n",
    "            maxDepth=15,\n",
    "            seed=42\n",
    "        )\n",
    "    else:\n",
    "        model = DecisionTreeClassifier(\n",
    "            featuresCol=\"features\",\n",
    "            labelCol=TARGET_COL,\n",
    "            maxDepth=15,\n",
    "            seed=42\n",
    "        )\n",
    "\n",
    "    start_time = time.time()\n",
    "    fitted_model = model.fit(train_vec)\n",
    "    end_time = time.time()\n",
    "\n",
    "    total_runtime = end_time - start_time\n",
    "    print(f\"   Training completed in {total_runtime:.2f} seconds.\")\n",
    "\n",
    "    # Save model\n",
    "    print(\"\\n3. Saving Model...\")\n",
    "    fitted_model.write().overwrite().save(os.path.join(model_output_dir, \"spark_model\"))\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    print(\"\\n4. Evaluating on Validation Set...\")\n",
    "    val_preds = fitted_model.transform(val_vec)\n",
    "    accuracy = evaluate_model(val_preds, TARGET_COL, MODEL_TYPE)\n",
    "    \n",
    "    metric_name = \"RMSE\" if MODEL_TYPE == \"regression\" else \"Accuracy\"\n",
    "    print(f\"   Validation {metric_name}: {accuracy:.4f}\")\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SUMMARY:\")\n",
    "    print(f\"  Cores: {cores}\")\n",
    "    print(f\"  Data fraction: {pct}%\")\n",
    "    print(f\"  Training rows: {train_count:,}\")\n",
    "    print(f\"  Total runtime: {total_runtime:.2f}s\")\n",
    "    print(f\"  Validation {metric_name}: {accuracy:.4f}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "    # Log results to CSV\n",
    "    with open(filename, \"a\") as f:\n",
    "        # Format: cores, pct, total_runtime, train_count, accuracy\n",
    "        print(f\"{cores},{pct},{total_runtime},{train_count},{accuracy}\", file=f)\n",
    "\n",
    "    # Cleanup\n",
    "    train_vec.unpersist()\n",
    "    spark.stop()\n",
    "    \n",
    "    print(\"--- PROCESS FINISHED ---\\n\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Entry point\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    main(sys.argv[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c321a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STARTING: RandomForest_New_Features ---\n",
      "1. Loading Data...\n",
      "2. Applying Feature Engineering (Month + Solid_PRCP)...\n",
      "   Features (21): ['LATITUDE', 'LONGITUDE', 'ELEVATION', 'TEMP', 'DEWP', 'SLP', 'STP', 'VISIB', 'WDSP', 'MXSPD', 'MAX', 'MIN', 'PRCP', 'is_Fog', 'is_Rain', 'is_Snow', 'is_Hail', 'is_Thunder', 'is_Tornado', 'MONTH', 'Solid_PRCP']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Training Rows: 51,241,804\n",
      "3. Training RandomForest_New_Features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Training completed in 1778.49 seconds.\n",
      "4. Processing Results...\n",
      "   [v] Feature Importance saved to: saved_models/RandomForest_New_Features/feature_importance.csv\n",
      "   --- TOP 5 FEATURES ---\n",
      "   Feature  Importance\n",
      "11     MIN    0.273749\n",
      "10     MAX    0.217890\n",
      "3     TEMP    0.105897\n",
      "19   MONTH    0.095371\n",
      "4     DEWP    0.073162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- METRICS (RandomForest_New_Features) ---\n",
      "R2: 0.6995 | RMSE: 6.2765 | R2 Snow Only: 0.4675\n",
      "   Generating plots...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   [v] Plots saved in: saved_models/RandomForest_New_Features\n",
      "\n",
      "--- PROCESS FINISHED ---\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"RandomForest_New_Features\"\n",
    "\n",
    "print(f\"--- STARTING: {MODEL_NAME} ---\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(MODEL_NAME) \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"10g\") \\\n",
    "    .config(\"spark.executor.memory\", \"10g\") \\\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"128m\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "model_output_dir = os.path.join(BASE_OUTPUT_DIR, MODEL_NAME)\n",
    "if os.path.exists(model_output_dir):\n",
    "    shutil.rmtree(model_output_dir)\n",
    "os.makedirs(model_output_dir)\n",
    "\n",
    "train_years = range(2010, 2021) \n",
    "val_years   = range(2021, 2023) \n",
    "test_years  = range(2023, 2025) \n",
    "\n",
    "print(\"1. Loading Data...\")\n",
    "try:\n",
    "    train_raw = spark.read.parquet(*get_paths(train_years))\n",
    "    val_raw   = spark.read.parquet(*get_paths(val_years))\n",
    "    test_raw  = spark.read.parquet(*get_paths(test_years))\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    spark.stop()\n",
    "    raise e\n",
    "\n",
    "print(\"2. Applying Feature Engineering (Month + Solid_PRCP)...\")\n",
    "\n",
    "def add_smart_features(df):\n",
    "    df = df.withColumn(\"MONTH\", month(col(\"DATE\")))\n",
    "    df = df.withColumn(\n",
    "        \"Solid_PRCP\", \n",
    "        when((col(\"PRCP\") > 0) & (col(\"TEMP\") < 2.0), col(\"PRCP\")).otherwise(0.0)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "train_df = add_smart_features(train_raw)\n",
    "val_df   = add_smart_features(val_raw)\n",
    "test_df  = add_smart_features(test_raw)\n",
    "\n",
    "target_col = \"SNDP\"\n",
    "ignore_cols = [target_col, \"DATE\", \"STATION\", \"NAME\", \"features\", \"prediction\", \"FRSHTT\"]\n",
    "valid_types = ['int', 'bigint', 'float', 'double', 'tinyint', 'smallint']\n",
    "\n",
    "dtypes = train_df.dtypes\n",
    "feature_cols = [c for c, t in dtypes if t in valid_types and c not in ignore_cols]\n",
    "print(f\"   Features ({len(feature_cols)}): {feature_cols}\")\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\").setHandleInvalid(\"skip\")\n",
    "\n",
    "train_vec = assembler.transform(train_df)\n",
    "val_vec   = assembler.transform(val_df)\n",
    "test_vec  = assembler.transform(test_df)\n",
    "\n",
    "train_vec.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "print(f\"   Training Rows: {train_vec.count():,}\")\n",
    "\n",
    "print(f\"3. Training {MODEL_NAME}...\")\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "    featuresCol=\"features\", \n",
    "    labelCol=target_col,\n",
    "    numTrees=40,         \n",
    "    maxDepth=10,         \n",
    "    seed=42,\n",
    "    subsamplingRate=0.7 \n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "model = rf.fit(train_vec)\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "\n",
    "print(f\"   Training completed in {duration:.2f} seconds.\")\n",
    "\n",
    "print(\"4. Processing Results...\")\n",
    "\n",
    "model.write().overwrite().save(os.path.join(model_output_dir, \"spark_model\"))\n",
    "save_feature_importance(model, feature_cols, model_output_dir)\n",
    "\n",
    "test_preds = model.transform(test_vec)\n",
    "metrics = evaluate_and_log(test_preds, target_col, duration, model_output_dir, MODEL_NAME)\n",
    "\n",
    "plot_predictions(test_preds, target_col, model_output_dir, MODEL_NAME)\n",
    "\n",
    "print(\"\\n--- PROCESS FINISHED ---\")\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deb9ec0",
   "metadata": {},
   "source": [
    "## GBT Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a52cdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STARTING: GBT_New_Features ---\n",
      "1. Loading Data...\n",
      "2. Applying Feature Engineering (Month + Solid_PRCP)...\n",
      "   Features (21): ['LATITUDE', 'LONGITUDE', 'ELEVATION', 'TEMP', 'DEWP', 'SLP', 'STP', 'VISIB', 'WDSP', 'MXSPD', 'MAX', 'MIN', 'PRCP', 'is_Fog', 'is_Rain', 'is_Snow', 'is_Hail', 'is_Thunder', 'is_Tornado', 'MONTH', 'Solid_PRCP']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Training Rows: 51,241,804\n",
      "3. Training GBT_New_Features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Training completed in 8294.65 seconds.\n",
      "4. Processing Results...\n",
      "   [v] Feature Importance saved to: saved_models/GBT_New_Features/feature_importance.csv\n",
      "   --- TOP 5 FEATURES ---\n",
      "      Feature  Importance\n",
      "10        MAX    0.270098\n",
      "1   LONGITUDE    0.170592\n",
      "0    LATITUDE    0.152985\n",
      "2   ELEVATION    0.143522\n",
      "19      MONTH    0.111063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- METRICS (GBT_New_Features) ---\n",
      "R2: 0.6998 | RMSE: 6.2742 | R2 Snow Only: 0.4696\n",
      "   Generating plots...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   [v] Plots saved in: saved_models/GBT_New_Features\n",
      "\n",
      "--- PROCESS FINISHED ---\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"GBT_New_Features\"\n",
    "\n",
    "print(f\"--- STARTING: {MODEL_NAME} ---\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(MODEL_NAME) \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"10g\") \\\n",
    "    .config(\"spark.executor.memory\", \"10g\") \\\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"128m\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "model_output_dir = os.path.join(BASE_OUTPUT_DIR, MODEL_NAME)\n",
    "if os.path.exists(model_output_dir):\n",
    "    shutil.rmtree(model_output_dir)\n",
    "os.makedirs(model_output_dir)\n",
    "\n",
    "train_years = range(2010, 2021) \n",
    "val_years   = range(2021, 2023) \n",
    "test_years  = range(2023, 2025) \n",
    "\n",
    "print(\"1. Loading Data...\")\n",
    "try:\n",
    "    train_raw = spark.read.parquet(*get_paths(train_years))\n",
    "    val_raw   = spark.read.parquet(*get_paths(val_years))\n",
    "    test_raw  = spark.read.parquet(*get_paths(test_years))\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    spark.stop()\n",
    "    raise e\n",
    "\n",
    "print(\"2. Applying Feature Engineering (Month + Solid_PRCP)...\")\n",
    "\n",
    "def add_smart_features(df):\n",
    "    df = df.withColumn(\"MONTH\", month(col(\"DATE\")))\n",
    "    df = df.withColumn(\n",
    "        \"Solid_PRCP\", \n",
    "        when((col(\"PRCP\") > 0) & (col(\"TEMP\") < 2.0), col(\"PRCP\")).otherwise(0.0)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "train_df = add_smart_features(train_raw)\n",
    "val_df   = add_smart_features(val_raw)\n",
    "test_df  = add_smart_features(test_raw)\n",
    "\n",
    "target_col = \"SNDP\"\n",
    "ignore_cols = [target_col, \"DATE\", \"STATION\", \"NAME\", \"features\", \"prediction\", \"FRSHTT\"]\n",
    "valid_types = ['int', 'bigint', 'float', 'double', 'tinyint', 'smallint']\n",
    "\n",
    "dtypes = train_df.dtypes\n",
    "feature_cols = [c for c, t in dtypes if t in valid_types and c not in ignore_cols]\n",
    "print(f\"   Features ({len(feature_cols)}): {feature_cols}\")\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\").setHandleInvalid(\"skip\")\n",
    "\n",
    "train_vec = assembler.transform(train_df)\n",
    "val_vec   = assembler.transform(val_df)\n",
    "test_vec  = assembler.transform(test_df)\n",
    "\n",
    "train_vec.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "print(f\"   Training Rows: {train_vec.count():,}\")\n",
    "\n",
    "print(f\"3. Training {MODEL_NAME}...\")\n",
    "\n",
    "gbt = GBTRegressor(\n",
    "    featuresCol=\"features\", \n",
    "    labelCol=target_col,\n",
    "    maxIter=50,         \n",
    "    maxDepth=5,         \n",
    "    stepSize=0.1,\n",
    "    seed=42,\n",
    "    subsamplingRate=0.7 \n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "model = gbt.fit(train_vec)\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "\n",
    "print(f\"   Training completed in {duration:.2f} seconds.\")\n",
    "\n",
    "print(\"4. Processing Results...\")\n",
    "\n",
    "model.write().overwrite().save(os.path.join(model_output_dir, \"spark_model\"))\n",
    "save_feature_importance(model, feature_cols, model_output_dir)\n",
    "\n",
    "test_preds = model.transform(test_vec)\n",
    "metrics = evaluate_and_log(test_preds, target_col, duration, model_output_dir, MODEL_NAME)\n",
    "\n",
    "plot_predictions(test_preds, target_col, model_output_dir, MODEL_NAME)\n",
    "\n",
    "print(\"\\n--- PROCESS FINISHED ---\")\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6485c7a",
   "metadata": {},
   "source": [
    "## Hypertuning of Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3af9ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_OUTPUT_DIR = \"optimized_models\"     \n",
    "MASTER_LOG_FILE = \"tuning_comparison.csv\" \n",
    "BASE_DATA_PATH = \"processed_data\"    \n",
    "\n",
    "def get_paths(years):\n",
    "    return [f\"{BASE_DATA_PATH}/climate_{y}.parquet\" for y in years]\n",
    "\n",
    "def save_best_params(cv_model, output_dir):\n",
    "    \"\"\"\n",
    "    Extracts and saves the best hyperparameters found by CrossValidator.\n",
    "    \"\"\"\n",
    "    rf_model = cv_model.bestModel\n",
    "    \n",
    "    params = {\n",
    "        \"numTrees\": rf_model.getNumTrees,\n",
    "        \"maxDepth\": rf_model.getOrDefault(\"maxDepth\"),\n",
    "        \"maxBins\": rf_model.getOrDefault(\"maxBins\"),\n",
    "        \"subsamplingRate\": rf_model.getOrDefault(\"subsamplingRate\")\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(output_dir, \"best_params.json\"), 'w') as f:\n",
    "        json.dump(params, f, indent=4)\n",
    "    \n",
    "    print(\"\\n   [v] WINNING PARAMETERS SAVED:\")\n",
    "    print(json.dumps(params, indent=4))\n",
    "\n",
    "def evaluate_and_log(predictions, target_col, time_taken, output_dir, model_name):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the final Test set.\n",
    "    \"\"\"\n",
    "    evaluator = RegressionEvaluator(labelCol=target_col, predictionCol=\"prediction\")\n",
    "    \n",
    "    r2 = evaluator.setMetricName(\"r2\").evaluate(predictions)\n",
    "    rmse = evaluator.setMetricName(\"rmse\").evaluate(predictions)\n",
    "    \n",
    "    snow_subset = predictions.filter(col(target_col) > 0)\n",
    "    r2_snow = evaluator.setMetricName(\"r2\").evaluate(snow_subset) if snow_subset.count() > 0 else 0.0\n",
    "\n",
    "    metrics = {\n",
    "        \"Model\": model_name,\n",
    "        \"Time_Min\": round(time_taken / 60, 2),\n",
    "        \"R2_Global\": round(r2, 4),\n",
    "        \"RMSE_Global\": round(rmse, 4),\n",
    "        \"R2_Snow_Only\": round(r2_snow, 4)\n",
    "    }\n",
    "\n",
    "    file_exists = os.path.isfile(MASTER_LOG_FILE)\n",
    "    with open(MASTER_LOG_FILE, mode='a', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=metrics.keys())\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(metrics)\n",
    "\n",
    "    print(f\"\\n--- FINAL TEST RESULTS ({model_name}) ---\")\n",
    "    print(f\"R2 Global: {r2:.4f} | R2 Snow Only: {r2_snow:.4f}\")\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "414248fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STARTING TUNING: RandomForest_HyperTuning ---\n",
      "1. Loading and Transforming Data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Features to use: ['LATITUDE', 'LONGITUDE', 'ELEVATION', 'TEMP', 'DEWP', 'SLP', 'STP', 'VISIB', 'WDSP', 'MXSPD', 'MAX', 'MIN', 'PRCP', 'is_Fog', 'is_Rain', 'is_Snow', 'is_Hail', 'is_Thunder', 'is_Tornado', 'MONTH', 'Solid_PRCP']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Training Rows (Internal Train + Val): 60,908,682\n",
      "2. Configuring Grid Search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Original Training Rows: 60,908,682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Tuning Sample Rows: 12,185,649\n",
      "   Combinations to test: 2\n",
      "3. Running Train-Validation Split (Please wait)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/03 16:36:08 ERROR Executor: Exception in task 7.0 in stage 63.0 (TID 326)]\n",
      "java.io.IOException: No space left on device\n",
      "\tat java.base/sun.nio.ch.UnixFileDispatcherImpl.write0(Native Method)\n",
      "\tat java.base/sun.nio.ch.UnixFileDispatcherImpl.write(UnixFileDispatcherImpl.java:65)\n",
      "\tat java.base/sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:137)\n",
      "\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:102)\n",
      "\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:72)\n",
      "\tat java.base/sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:300)\n",
      "\tat org.apache.spark.storage.CountingWritableChannel.write(DiskStore.scala:370)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeFully$1(ChunkedByteBuffer.scala:90)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeFully$1$adapted(ChunkedByteBuffer.scala:78)\n",
      "\tat scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1324)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer.writeFully(ChunkedByteBuffer.scala:78)\n",
      "\tat org.apache.spark.storage.DiskStore.$anonfun$putBytes$1(DiskStore.scala:114)\n",
      "\tat org.apache.spark.storage.DiskStore.$anonfun$putBytes$1$adapted(DiskStore.scala:113)\n",
      "\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:90)\n",
      "\tat org.apache.spark.storage.DiskStore.putBytes(DiskStore.scala:113)\n",
      "\tat org.apache.spark.storage.BlockManager.dropFromMemory(BlockManager.scala:2000)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.dropBlock$1(MemoryStore.scala:513)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.$anonfun$evictBlocksToFreeSpace$4(MemoryStore.scala:539)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.evictBlocksToFreeSpace(MemoryStore.scala:530)\n",
      "\tat org.apache.spark.memory.StorageMemoryPool.acquireMemory(StorageMemoryPool.scala:94)\n",
      "\tat org.apache.spark.memory.StorageMemoryPool.acquireMemory(StorageMemoryPool.scala:75)\n",
      "\tat org.apache.spark.memory.UnifiedMemoryManager.acquireStorageMemory(UnifiedMemoryManager.scala:183)\n",
      "\tat org.apache.spark.memory.UnifiedMemoryManager.acquireUnrollMemory(UnifiedMemoryManager.scala:190)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.reserveUnrollMemoryForThisTask(MemoryStore.scala:593)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:241)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:319)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1634)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1560)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1625)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1424)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1378)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:386)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "26/01/03 16:36:08 ERROR TaskSetManager: Task 7 in stage 63.0 failed 1 times; aborting job\n",
      "26/01/03 16:36:08 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 63.0 failed 1 times, most recent failure: Lost task 7.0 in stage 63.0 (TID 326) (eim-alu-83086.lab.unavarra.es executor driver): java.io.IOException: No space left on device\n",
      "\tat java.base/sun.nio.ch.UnixFileDispatcherImpl.write0(Native Method)\n",
      "\tat java.base/sun.nio.ch.UnixFileDispatcherImpl.write(UnixFileDispatcherImpl.java:65)\n",
      "\tat java.base/sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:137)\n",
      "\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:102)\n",
      "\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:72)\n",
      "\tat java.base/sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:300)\n",
      "\tat org.apache.spark.storage.CountingWritableChannel.write(DiskStore.scala:370)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeFully$1(ChunkedByteBuffer.scala:90)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeFully$1$adapted(ChunkedByteBuffer.scala:78)\n",
      "\tat scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1324)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer.writeFully(ChunkedByteBuffer.scala:78)\n",
      "\tat org.apache.spark.storage.DiskStore.$anonfun$putBytes$1(DiskStore.scala:114)\n",
      "\tat org.apache.spark.storage.DiskStore.$anonfun$putBytes$1$adapted(DiskStore.scala:113)\n",
      "\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:90)\n",
      "\tat org.apache.spark.storage.DiskStore.putBytes(DiskStore.scala:113)\n",
      "\tat org.apache.spark.storage.BlockManager.dropFromMemory(BlockManager.scala:2000)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.dropBlock$1(MemoryStore.scala:513)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.$anonfun$evictBlocksToFreeSpace$4(MemoryStore.scala:539)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.evictBlocksToFreeSpace(MemoryStore.scala:530)\n",
      "\tat org.apache.spark.memory.StorageMemoryPool.acquireMemory(StorageMemoryPool.scala:94)\n",
      "\tat org.apache.spark.memory.StorageMemoryPool.acquireMemory(StorageMemoryPool.scala:75)\n",
      "\tat org.apache.spark.memory.UnifiedMemoryManager.acquireStorageMemory(UnifiedMemoryManager.scala:183)\n",
      "\tat org.apache.spark.memory.UnifiedMemoryManager.acquireUnrollMemory(UnifiedMemoryManager.scala:190)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.reserveUnrollMemoryForThisTask(MemoryStore.scala:593)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:241)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:319)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1634)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1560)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1625)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1424)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1378)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:386)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\n",
      "\tat scala.Option.foreach(Option.scala:437)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2549)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1057)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1056)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:740)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:739)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:665)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.runBagged(RandomForest.scala:210)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:304)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.$anonfun$train$1(RandomForestRegressor.scala:159)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:226)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:226)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:137)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:46)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:79)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.io.IOException: No space left on device\n",
      "\tat java.base/sun.nio.ch.UnixFileDispatcherImpl.write0(Native Method)\n",
      "\tat java.base/sun.nio.ch.UnixFileDispatcherImpl.write(UnixFileDispatcherImpl.java:65)\n",
      "\tat java.base/sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:137)\n",
      "\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:102)\n",
      "\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:72)\n",
      "\tat java.base/sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:300)\n",
      "\tat org.apache.spark.storage.CountingWritableChannel.write(DiskStore.scala:370)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeFully$1(ChunkedByteBuffer.scala:90)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeFully$1$adapted(ChunkedByteBuffer.scala:78)\n",
      "\tat scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1324)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer.writeFully(ChunkedByteBuffer.scala:78)\n",
      "\tat org.apache.spark.storage.DiskStore.$anonfun$putBytes$1(DiskStore.scala:114)\n",
      "\tat org.apache.spark.storage.DiskStore.$anonfun$putBytes$1$adapted(DiskStore.scala:113)\n",
      "\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:90)\n",
      "\tat org.apache.spark.storage.DiskStore.putBytes(DiskStore.scala:113)\n",
      "\tat org.apache.spark.storage.BlockManager.dropFromMemory(BlockManager.scala:2000)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.dropBlock$1(MemoryStore.scala:513)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.$anonfun$evictBlocksToFreeSpace$4(MemoryStore.scala:539)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.evictBlocksToFreeSpace(MemoryStore.scala:530)\n",
      "\tat org.apache.spark.memory.StorageMemoryPool.acquireMemory(StorageMemoryPool.scala:94)\n",
      "\tat org.apache.spark.memory.StorageMemoryPool.acquireMemory(StorageMemoryPool.scala:75)\n",
      "\tat org.apache.spark.memory.UnifiedMemoryManager.acquireStorageMemory(UnifiedMemoryManager.scala:183)\n",
      "\tat org.apache.spark.memory.UnifiedMemoryManager.acquireUnrollMemory(UnifiedMemoryManager.scala:190)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.reserveUnrollMemoryForThisTask(MemoryStore.scala:593)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:241)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:319)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1634)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1560)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1625)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1424)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1378)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:386)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\t... 1 more\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2006.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 63.0 failed 1 times, most recent failure: Lost task 7.0 in stage 63.0 (TID 326) (eim-alu-83086.lab.unavarra.es executor driver): java.io.IOException: No space left on device\n\tat java.base/sun.nio.ch.UnixFileDispatcherImpl.write0(Native Method)\n\tat java.base/sun.nio.ch.UnixFileDispatcherImpl.write(UnixFileDispatcherImpl.java:65)\n\tat java.base/sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:137)\n\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:102)\n\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:72)\n\tat java.base/sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:300)\n\tat org.apache.spark.storage.CountingWritableChannel.write(DiskStore.scala:370)\n\tat org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeFully$1(ChunkedByteBuffer.scala:90)\n\tat org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeFully$1$adapted(ChunkedByteBuffer.scala:78)\n\tat scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1324)\n\tat org.apache.spark.util.io.ChunkedByteBuffer.writeFully(ChunkedByteBuffer.scala:78)\n\tat org.apache.spark.storage.DiskStore.$anonfun$putBytes$1(DiskStore.scala:114)\n\tat org.apache.spark.storage.DiskStore.$anonfun$putBytes$1$adapted(DiskStore.scala:113)\n\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:90)\n\tat org.apache.spark.storage.DiskStore.putBytes(DiskStore.scala:113)\n\tat org.apache.spark.storage.BlockManager.dropFromMemory(BlockManager.scala:2000)\n\tat org.apache.spark.storage.memory.MemoryStore.dropBlock$1(MemoryStore.scala:513)\n\tat org.apache.spark.storage.memory.MemoryStore.$anonfun$evictBlocksToFreeSpace$4(MemoryStore.scala:539)\n\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n\tat org.apache.spark.storage.memory.MemoryStore.evictBlocksToFreeSpace(MemoryStore.scala:530)\n\tat org.apache.spark.memory.StorageMemoryPool.acquireMemory(StorageMemoryPool.scala:94)\n\tat org.apache.spark.memory.StorageMemoryPool.acquireMemory(StorageMemoryPool.scala:75)\n\tat org.apache.spark.memory.UnifiedMemoryManager.acquireStorageMemory(UnifiedMemoryManager.scala:183)\n\tat org.apache.spark.memory.UnifiedMemoryManager.acquireUnrollMemory(UnifiedMemoryManager.scala:190)\n\tat org.apache.spark.storage.memory.MemoryStore.reserveUnrollMemoryForThisTask(MemoryStore.scala:593)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:241)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:319)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1634)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1560)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1625)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1424)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1378)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:386)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\n\tat scala.Option.foreach(Option.scala:437)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2549)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1057)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1056)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:740)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:739)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:665)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.runBagged(RandomForest.scala:210)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:304)\n\tat org.apache.spark.ml.regression.RandomForestRegressor.$anonfun$train$1(RandomForestRegressor.scala:159)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:226)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:226)\n\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:137)\n\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:46)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:79)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: java.io.IOException: No space left on device\n\tat java.base/sun.nio.ch.UnixFileDispatcherImpl.write0(Native Method)\n\tat java.base/sun.nio.ch.UnixFileDispatcherImpl.write(UnixFileDispatcherImpl.java:65)\n\tat java.base/sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:137)\n\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:102)\n\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:72)\n\tat java.base/sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:300)\n\tat org.apache.spark.storage.CountingWritableChannel.write(DiskStore.scala:370)\n\tat org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeFully$1(ChunkedByteBuffer.scala:90)\n\tat org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeFully$1$adapted(ChunkedByteBuffer.scala:78)\n\tat scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1324)\n\tat org.apache.spark.util.io.ChunkedByteBuffer.writeFully(ChunkedByteBuffer.scala:78)\n\tat org.apache.spark.storage.DiskStore.$anonfun$putBytes$1(DiskStore.scala:114)\n\tat org.apache.spark.storage.DiskStore.$anonfun$putBytes$1$adapted(DiskStore.scala:113)\n\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:90)\n\tat org.apache.spark.storage.DiskStore.putBytes(DiskStore.scala:113)\n\tat org.apache.spark.storage.BlockManager.dropFromMemory(BlockManager.scala:2000)\n\tat org.apache.spark.storage.memory.MemoryStore.dropBlock$1(MemoryStore.scala:513)\n\tat org.apache.spark.storage.memory.MemoryStore.$anonfun$evictBlocksToFreeSpace$4(MemoryStore.scala:539)\n\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n\tat org.apache.spark.storage.memory.MemoryStore.evictBlocksToFreeSpace(MemoryStore.scala:530)\n\tat org.apache.spark.memory.StorageMemoryPool.acquireMemory(StorageMemoryPool.scala:94)\n\tat org.apache.spark.memory.StorageMemoryPool.acquireMemory(StorageMemoryPool.scala:75)\n\tat org.apache.spark.memory.UnifiedMemoryManager.acquireStorageMemory(UnifiedMemoryManager.scala:183)\n\tat org.apache.spark.memory.UnifiedMemoryManager.acquireUnrollMemory(UnifiedMemoryManager.scala:190)\n\tat org.apache.spark.storage.memory.MemoryStore.reserveUnrollMemoryForThisTask(MemoryStore.scala:593)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:241)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:319)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1634)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1560)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1625)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1424)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1378)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:386)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 110\u001b[39m\n\u001b[32m    107\u001b[39m start_time = time.time()\n\u001b[32m    109\u001b[39m \u001b[38;5;66;03m# Pass the SAMPLE to the tuner\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m cv_model = \u001b[43mtvs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_vec_sample\u001b[49m\u001b[43m)\u001b[49m \n\u001b[32m    112\u001b[39m end_time = time.time()\n\u001b[32m    113\u001b[39m duration = end_time - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py313ml/.venv/lib/python3.13/site-packages/pyspark/ml/base.py:203\u001b[39m, in \u001b[36mEstimator.fit\u001b[39m\u001b[34m(self, dataset, params)\u001b[39m\n\u001b[32m    201\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy(params)._fit(dataset)\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    205\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    206\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    207\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m % \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[32m    208\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py313ml/.venv/lib/python3.13/site-packages/pyspark/ml/tuning.py:1497\u001b[39m, in \u001b[36mTrainValidationSplit._fit\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m   1495\u001b[39m pool = ThreadPool(processes=\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m.getParallelism(), numModels))\n\u001b[32m   1496\u001b[39m metrics = [\u001b[38;5;28;01mNone\u001b[39;00m] * numModels\n\u001b[32m-> \u001b[39m\u001b[32m1497\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubModel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimap_unordered\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1498\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\n\u001b[32m   1499\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcollectSubModelsParam\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.7-linux-x86_64-gnu/lib/python3.13/multiprocessing/pool.py:873\u001b[39m, in \u001b[36mIMapIterator.next\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    871\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m    872\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.7-linux-x86_64-gnu/lib/python3.13/multiprocessing/pool.py:125\u001b[39m, in \u001b[36mworker\u001b[39m\u001b[34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[39m\n\u001b[32m    123\u001b[39m job, i, func, args, kwds = task\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     result = (\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m wrap_exception \u001b[38;5;129;01mand\u001b[39;00m func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _helper_reraises_exception:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py313ml/.venv/lib/python3.13/site-packages/pyspark/ml/tuning.py:1497\u001b[39m, in \u001b[36mTrainValidationSplit._fit.<locals>.<lambda>\u001b[39m\u001b[34m(f)\u001b[39m\n\u001b[32m   1495\u001b[39m pool = ThreadPool(processes=\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m.getParallelism(), numModels))\n\u001b[32m   1496\u001b[39m metrics = [\u001b[38;5;28;01mNone\u001b[39;00m] * numModels\n\u001b[32m-> \u001b[39m\u001b[32m1497\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m j, metric, subModel \u001b[38;5;129;01min\u001b[39;00m pool.imap_unordered(\u001b[38;5;28;01mlambda\u001b[39;00m f: \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, tasks):\n\u001b[32m   1498\u001b[39m     metrics[j] = metric\n\u001b[32m   1499\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m collectSubModelsParam:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py313ml/.venv/lib/python3.13/site-packages/pyspark/util.py:435\u001b[39m, in \u001b[36minheritable_thread_target.<locals>.outer.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    433\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m tag \u001b[38;5;129;01min\u001b[39;00m tags:\n\u001b[32m    434\u001b[39m     session.addTag(tag)  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m435\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mff\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py313ml/.venv/lib/python3.13/site-packages/pyspark/ml/tuning.py:115\u001b[39m, in \u001b[36m_parallelFitTasks.<locals>.singleTask\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msingleTask\u001b[39m() -> Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, Transformer]:\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     index, model = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodelIter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m     \u001b[38;5;66;03m# TODO: duplicate evaluator to take extra params from input\u001b[39;00m\n\u001b[32m    117\u001b[39m     \u001b[38;5;66;03m#  Note: Supporting tuning params in evaluator need update method\u001b[39;00m\n\u001b[32m    118\u001b[39m     \u001b[38;5;66;03m#  `MetaAlgorithmReadWrite.getAllNestedStages`, make it return\u001b[39;00m\n\u001b[32m    119\u001b[39m     \u001b[38;5;66;03m#  all nested stages and evaluators\u001b[39;00m\n\u001b[32m    120\u001b[39m     metric = eva.evaluate(model.transform(validation, epm[index]))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py313ml/.venv/lib/python3.13/site-packages/pyspark/ml/base.py:96\u001b[39m, in \u001b[36m_FitMultipleIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     94\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo models remaining.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     95\u001b[39m     \u001b[38;5;28mself\u001b[39m.counter += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m index, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfitSingleModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py313ml/.venv/lib/python3.13/site-packages/pyspark/ml/base.py:154\u001b[39m, in \u001b[36mEstimator.fitMultiple.<locals>.fitSingleModel\u001b[39m\u001b[34m(index)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfitSingleModel\u001b[39m(index: \u001b[38;5;28mint\u001b[39m) -> M:\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparamMaps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py313ml/.venv/lib/python3.13/site-packages/pyspark/ml/base.py:201\u001b[39m, in \u001b[36mEstimator.fit\u001b[39m\u001b[34m(self, dataset, params)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(params, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m params:\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    203\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fit(dataset)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py313ml/.venv/lib/python3.13/site-packages/pyspark/ml/util.py:164\u001b[39m, in \u001b[36mtry_remote_fit.<locals>.wrapped\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    162\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._copyValues(model)\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py313ml/.venv/lib/python3.13/site-packages/pyspark/ml/wrapper.py:411\u001b[39m, in \u001b[36mJavaEstimator._fit\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    409\u001b[39m \u001b[38;5;129m@try_remote_fit\u001b[39m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) -> JM:\n\u001b[32m--> \u001b[39m\u001b[32m411\u001b[39m     java_model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    412\u001b[39m     model = \u001b[38;5;28mself\u001b[39m._create_model(java_model)\n\u001b[32m    413\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._copyValues(model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py313ml/.venv/lib/python3.13/site-packages/pyspark/ml/wrapper.py:407\u001b[39m, in \u001b[36mJavaEstimator._fit_java\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    404\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    406\u001b[39m \u001b[38;5;28mself\u001b[39m._transfer_params_to_java()\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_java_obj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py313ml/.venv/lib/python3.13/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py313ml/.venv/lib/python3.13/site-packages/pyspark/errors/exceptions/captured.py:282\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpy4j\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    284\u001b[39m     converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py313ml/.venv/lib/python3.13/site-packages/py4j/protocol.py:327\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    325\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    332\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    333\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o2006.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 63.0 failed 1 times, most recent failure: Lost task 7.0 in stage 63.0 (TID 326) (eim-alu-83086.lab.unavarra.es executor driver): java.io.IOException: No space left on device\n\tat java.base/sun.nio.ch.UnixFileDispatcherImpl.write0(Native Method)\n\tat java.base/sun.nio.ch.UnixFileDispatcherImpl.write(UnixFileDispatcherImpl.java:65)\n\tat java.base/sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:137)\n\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:102)\n\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:72)\n\tat java.base/sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:300)\n\tat org.apache.spark.storage.CountingWritableChannel.write(DiskStore.scala:370)\n\tat org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeFully$1(ChunkedByteBuffer.scala:90)\n\tat org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeFully$1$adapted(ChunkedByteBuffer.scala:78)\n\tat scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1324)\n\tat org.apache.spark.util.io.ChunkedByteBuffer.writeFully(ChunkedByteBuffer.scala:78)\n\tat org.apache.spark.storage.DiskStore.$anonfun$putBytes$1(DiskStore.scala:114)\n\tat org.apache.spark.storage.DiskStore.$anonfun$putBytes$1$adapted(DiskStore.scala:113)\n\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:90)\n\tat org.apache.spark.storage.DiskStore.putBytes(DiskStore.scala:113)\n\tat org.apache.spark.storage.BlockManager.dropFromMemory(BlockManager.scala:2000)\n\tat org.apache.spark.storage.memory.MemoryStore.dropBlock$1(MemoryStore.scala:513)\n\tat org.apache.spark.storage.memory.MemoryStore.$anonfun$evictBlocksToFreeSpace$4(MemoryStore.scala:539)\n\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n\tat org.apache.spark.storage.memory.MemoryStore.evictBlocksToFreeSpace(MemoryStore.scala:530)\n\tat org.apache.spark.memory.StorageMemoryPool.acquireMemory(StorageMemoryPool.scala:94)\n\tat org.apache.spark.memory.StorageMemoryPool.acquireMemory(StorageMemoryPool.scala:75)\n\tat org.apache.spark.memory.UnifiedMemoryManager.acquireStorageMemory(UnifiedMemoryManager.scala:183)\n\tat org.apache.spark.memory.UnifiedMemoryManager.acquireUnrollMemory(UnifiedMemoryManager.scala:190)\n\tat org.apache.spark.storage.memory.MemoryStore.reserveUnrollMemoryForThisTask(MemoryStore.scala:593)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:241)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:319)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1634)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1560)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1625)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1424)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1378)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:386)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\n\tat scala.Option.foreach(Option.scala:437)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2549)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1057)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1056)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:740)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:739)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:665)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.runBagged(RandomForest.scala:210)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:304)\n\tat org.apache.spark.ml.regression.RandomForestRegressor.$anonfun$train$1(RandomForestRegressor.scala:159)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:226)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:226)\n\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:137)\n\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:46)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:79)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: java.io.IOException: No space left on device\n\tat java.base/sun.nio.ch.UnixFileDispatcherImpl.write0(Native Method)\n\tat java.base/sun.nio.ch.UnixFileDispatcherImpl.write(UnixFileDispatcherImpl.java:65)\n\tat java.base/sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:137)\n\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:102)\n\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:72)\n\tat java.base/sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:300)\n\tat org.apache.spark.storage.CountingWritableChannel.write(DiskStore.scala:370)\n\tat org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeFully$1(ChunkedByteBuffer.scala:90)\n\tat org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeFully$1$adapted(ChunkedByteBuffer.scala:78)\n\tat scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1324)\n\tat org.apache.spark.util.io.ChunkedByteBuffer.writeFully(ChunkedByteBuffer.scala:78)\n\tat org.apache.spark.storage.DiskStore.$anonfun$putBytes$1(DiskStore.scala:114)\n\tat org.apache.spark.storage.DiskStore.$anonfun$putBytes$1$adapted(DiskStore.scala:113)\n\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:90)\n\tat org.apache.spark.storage.DiskStore.putBytes(DiskStore.scala:113)\n\tat org.apache.spark.storage.BlockManager.dropFromMemory(BlockManager.scala:2000)\n\tat org.apache.spark.storage.memory.MemoryStore.dropBlock$1(MemoryStore.scala:513)\n\tat org.apache.spark.storage.memory.MemoryStore.$anonfun$evictBlocksToFreeSpace$4(MemoryStore.scala:539)\n\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n\tat org.apache.spark.storage.memory.MemoryStore.evictBlocksToFreeSpace(MemoryStore.scala:530)\n\tat org.apache.spark.memory.StorageMemoryPool.acquireMemory(StorageMemoryPool.scala:94)\n\tat org.apache.spark.memory.StorageMemoryPool.acquireMemory(StorageMemoryPool.scala:75)\n\tat org.apache.spark.memory.UnifiedMemoryManager.acquireStorageMemory(UnifiedMemoryManager.scala:183)\n\tat org.apache.spark.memory.UnifiedMemoryManager.acquireUnrollMemory(UnifiedMemoryManager.scala:190)\n\tat org.apache.spark.storage.memory.MemoryStore.reserveUnrollMemoryForThisTask(MemoryStore.scala:593)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:241)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:319)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1634)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1560)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1625)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1424)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1378)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:386)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\t... 1 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/03 16:36:48 ERROR Executor: Exception in task 7.0 in stage 69.0 (TID 359)]\n",
      "java.io.IOException: No space left on device\n",
      "\tat java.base/sun.nio.ch.UnixFileDispatcherImpl.write0(Native Method)\n",
      "\tat java.base/sun.nio.ch.UnixFileDispatcherImpl.write(UnixFileDispatcherImpl.java:65)\n",
      "\tat java.base/sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:137)\n",
      "\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:102)\n",
      "\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:72)\n",
      "\tat java.base/sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:300)\n",
      "\tat org.apache.spark.storage.CountingWritableChannel.write(DiskStore.scala:370)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeFully$1(ChunkedByteBuffer.scala:90)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeFully$1$adapted(ChunkedByteBuffer.scala:78)\n",
      "\tat scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1324)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer.writeFully(ChunkedByteBuffer.scala:78)\n",
      "\tat org.apache.spark.storage.DiskStore.$anonfun$putBytes$1(DiskStore.scala:114)\n",
      "\tat org.apache.spark.storage.DiskStore.$anonfun$putBytes$1$adapted(DiskStore.scala:113)\n",
      "\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:90)\n",
      "\tat org.apache.spark.storage.DiskStore.putBytes(DiskStore.scala:113)\n",
      "\tat org.apache.spark.storage.BlockManager.dropFromMemory(BlockManager.scala:2000)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.dropBlock$1(MemoryStore.scala:513)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.$anonfun$evictBlocksToFreeSpace$4(MemoryStore.scala:539)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.evictBlocksToFreeSpace(MemoryStore.scala:530)\n",
      "\tat org.apache.spark.memory.StorageMemoryPool.acquireMemory(StorageMemoryPool.scala:94)\n",
      "\tat org.apache.spark.memory.StorageMemoryPool.acquireMemory(StorageMemoryPool.scala:75)\n",
      "\tat org.apache.spark.memory.UnifiedMemoryManager.acquireStorageMemory(UnifiedMemoryManager.scala:183)\n",
      "\tat org.apache.spark.memory.UnifiedMemoryManager.acquireUnrollMemory(UnifiedMemoryManager.scala:190)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.reserveUnrollMemoryForThisTask(MemoryStore.scala:593)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:241)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:319)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1634)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1560)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1625)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1424)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1378)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:386)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "26/01/03 16:36:48 ERROR Executor: Exception in task 6.0 in stage 69.0 (TID 358)\n",
      "java.io.IOException: No space left on device\n",
      "\tat java.base/sun.nio.ch.UnixFileDispatcherImpl.write0(Native Method)\n",
      "\tat java.base/sun.nio.ch.UnixFileDispatcherImpl.write(UnixFileDispatcherImpl.java:65)\n",
      "\tat java.base/sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:137)\n",
      "\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:102)\n",
      "\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:72)\n",
      "\tat java.base/sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:300)\n",
      "\tat org.apache.spark.storage.CountingWritableChannel.write(DiskStore.scala:370)\n",
      "\tat java.base/sun.nio.ch.ChannelOutputStream.writeFully(ChannelOutputStream.java:68)\n",
      "\tat java.base/sun.nio.ch.ChannelOutputStream.write(ChannelOutputStream.java:105)\n",
      "\tat java.base/java.io.BufferedOutputStream.implWrite(BufferedOutputStream.java:217)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:200)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
      "\tat java.base/java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1899)\n",
      "\tat java.base/java.io.ObjectOutputStream$BlockDataOutputStream.writeByte(ObjectOutputStream.java:1937)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeFatalException(ObjectOutputStream.java:1614)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:361)\n",
      "\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:47)\n",
      "\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n",
      "\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:177)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$dropFromMemory$3(BlockManager.scala:1997)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$dropFromMemory$3$adapted(BlockManager.scala:1992)\n",
      "\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:90)\n",
      "\tat org.apache.spark.storage.BlockManager.dropFromMemory(BlockManager.scala:1992)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.dropBlock$1(MemoryStore.scala:513)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.$anonfun$evictBlocksToFreeSpace$4(MemoryStore.scala:539)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.evictBlocksToFreeSpace(MemoryStore.scala:530)\n",
      "\tat org.apache.spark.memory.StorageMemoryPool.acquireMemory(StorageMemoryPool.scala:94)\n",
      "\tat org.apache.spark.memory.StorageMemoryPool.acquireMemory(StorageMemoryPool.scala:75)\n",
      "\tat org.apache.spark.memory.UnifiedMemoryManager.acquireStorageMemory(UnifiedMemoryManager.scala:183)\n",
      "\tat org.apache.spark.memory.UnifiedMemoryManager.acquireUnrollMemory(UnifiedMemoryManager.scala:190)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.reserveUnrollMemoryForThisTask(MemoryStore.scala:593)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:241)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:319)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1634)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1560)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1625)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1424)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1378)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:386)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "26/01/03 16:36:48 ERROR TaskSetManager: Task 7 in stage 69.0 failed 1 times; aborting job\n",
      "26/01/03 16:36:48 ERROR Executor: Exception in task 4.0 in stage 69.0 (TID 356)\n",
      "java.io.IOException: No space left on device\n",
      "\tat java.base/sun.nio.ch.UnixFileDispatcherImpl.write0(Native Method)\n",
      "\tat java.base/sun.nio.ch.UnixFileDispatcherImpl.write(UnixFileDispatcherImpl.java:65)\n",
      "\tat java.base/sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:137)\n",
      "\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:102)\n",
      "\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:72)\n",
      "\tat java.base/sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:300)\n",
      "\tat org.apache.spark.storage.CountingWritableChannel.write(DiskStore.scala:370)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeFully$1(ChunkedByteBuffer.scala:90)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeFully$1$adapted(ChunkedByteBuffer.scala:78)\n",
      "\tat scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1324)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer.writeFully(ChunkedByteBuffer.scala:78)\n",
      "\tat org.apache.spark.storage.DiskStore.$anonfun$putBytes$1(DiskStore.scala:114)\n",
      "\tat org.apache.spark.storage.DiskStore.$anonfun$putBytes$1$adapted(DiskStore.scala:113)\n",
      "\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:90)\n",
      "\tat org.apache.spark.storage.DiskStore.putBytes(DiskStore.scala:113)\n",
      "\tat org.apache.spark.storage.BlockManager.dropFromMemory(BlockManager.scala:2000)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.dropBlock$1(MemoryStore.scala:513)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.$anonfun$evictBlocksToFreeSpace$4(MemoryStore.scala:539)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.evictBlocksToFreeSpace(MemoryStore.scala:530)\n",
      "\tat org.apache.spark.memory.StorageMemoryPool.acquireMemory(StorageMemoryPool.scala:94)\n",
      "\tat org.apache.spark.memory.StorageMemoryPool.acquireMemory(StorageMemoryPool.scala:75)\n",
      "\tat org.apache.spark.memory.UnifiedMemoryManager.acquireStorageMemory(UnifiedMemoryManager.scala:183)\n",
      "\tat org.apache.spark.memory.UnifiedMemoryManager.acquireUnrollMemory(UnifiedMemoryManager.scala:190)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.reserveUnrollMemoryForThisTask(MemoryStore.scala:593)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:241)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:319)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1634)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1560)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1625)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1424)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1378)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:386)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "26/01/03 16:36:48 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 69.0 failed 1 times, most recent failure: Lost task 7.0 in stage 69.0 (TID 359) (eim-alu-83086.lab.unavarra.es executor driver): java.io.IOException: No space left on device\n",
      "\tat java.base/sun.nio.ch.UnixFileDispatcherImpl.write0(Native Method)\n",
      "\tat java.base/sun.nio.ch.UnixFileDispatcherImpl.write(UnixFileDispatcherImpl.java:65)\n",
      "\tat java.base/sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:137)\n",
      "\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:102)\n",
      "\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:72)\n",
      "\tat java.base/sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:300)\n",
      "\tat org.apache.spark.storage.CountingWritableChannel.write(DiskStore.scala:370)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeFully$1(ChunkedByteBuffer.scala:90)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeFully$1$adapted(ChunkedByteBuffer.scala:78)\n",
      "\tat scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1324)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer.writeFully(ChunkedByteBuffer.scala:78)\n",
      "\tat org.apache.spark.storage.DiskStore.$anonfun$putBytes$1(DiskStore.scala:114)\n",
      "\tat org.apache.spark.storage.DiskStore.$anonfun$putBytes$1$adapted(DiskStore.scala:113)\n",
      "\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:90)\n",
      "\tat org.apache.spark.storage.DiskStore.putBytes(DiskStore.scala:113)\n",
      "\tat org.apache.spark.storage.BlockManager.dropFromMemory(BlockManager.scala:2000)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.dropBlock$1(MemoryStore.scala:513)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.$anonfun$evictBlocksToFreeSpace$4(MemoryStore.scala:539)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.evictBlocksToFreeSpace(MemoryStore.scala:530)\n",
      "\tat org.apache.spark.memory.StorageMemoryPool.acquireMemory(StorageMemoryPool.scala:94)\n",
      "\tat org.apache.spark.memory.StorageMemoryPool.acquireMemory(StorageMemoryPool.scala:75)\n",
      "\tat org.apache.spark.memory.UnifiedMemoryManager.acquireStorageMemory(UnifiedMemoryManager.scala:183)\n",
      "\tat org.apache.spark.memory.UnifiedMemoryManager.acquireUnrollMemory(UnifiedMemoryManager.scala:190)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.reserveUnrollMemoryForThisTask(MemoryStore.scala:593)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:241)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:319)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1634)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1560)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1625)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1424)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1378)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:386)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\n",
      "\tat scala.Option.foreach(Option.scala:437)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2549)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1057)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1056)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:740)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:739)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:665)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.runBagged(RandomForest.scala:210)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:304)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.$anonfun$train$1(RandomForestRegressor.scala:159)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:226)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:226)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:137)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:46)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:79)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.io.IOException: No space left on device\n",
      "\tat java.base/sun.nio.ch.UnixFileDispatcherImpl.write0(Native Method)\n",
      "\tat java.base/sun.nio.ch.UnixFileDispatcherImpl.write(UnixFileDispatcherImpl.java:65)\n",
      "\tat java.base/sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:137)\n",
      "\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:102)\n",
      "\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:72)\n",
      "\tat java.base/sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:300)\n",
      "\tat org.apache.spark.storage.CountingWritableChannel.write(DiskStore.scala:370)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeFully$1(ChunkedByteBuffer.scala:90)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeFully$1$adapted(ChunkedByteBuffer.scala:78)\n",
      "\tat scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1324)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer.writeFully(ChunkedByteBuffer.scala:78)\n",
      "\tat org.apache.spark.storage.DiskStore.$anonfun$putBytes$1(DiskStore.scala:114)\n",
      "\tat org.apache.spark.storage.DiskStore.$anonfun$putBytes$1$adapted(DiskStore.scala:113)\n",
      "\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:90)\n",
      "\tat org.apache.spark.storage.DiskStore.putBytes(DiskStore.scala:113)\n",
      "\tat org.apache.spark.storage.BlockManager.dropFromMemory(BlockManager.scala:2000)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.dropBlock$1(MemoryStore.scala:513)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.$anonfun$evictBlocksToFreeSpace$4(MemoryStore.scala:539)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.evictBlocksToFreeSpace(MemoryStore.scala:530)\n",
      "\tat org.apache.spark.memory.StorageMemoryPool.acquireMemory(StorageMemoryPool.scala:94)\n",
      "\tat org.apache.spark.memory.StorageMemoryPool.acquireMemory(StorageMemoryPool.scala:75)\n",
      "\tat org.apache.spark.memory.UnifiedMemoryManager.acquireStorageMemory(UnifiedMemoryManager.scala:183)\n",
      "\tat org.apache.spark.memory.UnifiedMemoryManager.acquireUnrollMemory(UnifiedMemoryManager.scala:190)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.reserveUnrollMemoryForThisTask(MemoryStore.scala:593)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:241)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:319)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1634)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1560)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1625)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1424)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1378)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:386)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\t... 1 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"RandomForest_HyperTuning\"\n",
    "print(f\"--- STARTING TUNING: {MODEL_NAME} ---\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(MODEL_NAME) \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"12g\") \\\n",
    "    .config(\"spark.executor.memory\", \"12g\") \\\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"128m\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "model_output_dir = os.path.join(BASE_OUTPUT_DIR, MODEL_NAME)\n",
    "if os.path.exists(model_output_dir):\n",
    "    shutil.rmtree(model_output_dir)\n",
    "os.makedirs(model_output_dir)\n",
    "\n",
    "train_years = range(2010, 2023) \n",
    "test_years  = range(2023, 2025)\n",
    "\n",
    "print(\"1. Loading and Transforming Data...\")\n",
    "train_raw = spark.read.parquet(*get_paths(train_years))\n",
    "test_raw  = spark.read.parquet(*get_paths(test_years))\n",
    "\n",
    "def add_features(df):\n",
    "    df = df.withColumn(\"MONTH\", month(col(\"DATE\")))\n",
    "    df = df.withColumn(\"Solid_PRCP\", when((col(\"PRCP\") > 0) & (col(\"TEMP\") < 2.0), col(\"PRCP\")).otherwise(0.0))\n",
    "    return df\n",
    "\n",
    "train_df = add_features(train_raw)\n",
    "test_df  = add_features(test_raw)\n",
    "\n",
    "target_col = \"SNDP\"\n",
    "ignore_cols = [target_col, \"DATE\", \"STATION\", \"NAME\", \"features\", \"prediction\", \"FRSHTT\"]\n",
    "valid_types = ['int', 'bigint', 'float', 'double', 'tinyint', 'smallint']\n",
    "\n",
    "dtypes = train_df.dtypes\n",
    "feature_cols = [c for c, t in dtypes if t in valid_types and c not in ignore_cols]\n",
    "print(f\"   Features to use: {feature_cols}\")\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\").setHandleInvalid(\"skip\")\n",
    "\n",
    "train_vec = assembler.transform(train_df)\n",
    "test_vec  = assembler.transform(test_df)\n",
    "\n",
    "train_vec.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "print(f\"   Training Rows (Internal Train + Val): {train_vec.count():,}\")\n",
    "\n",
    "print(\"2. Configuring Grid Search...\")\n",
    "\n",
    "print(f\"   Original Training Rows: {train_vec.count():,}\")\n",
    "\n",
    "# --- ADD THIS SAMPLING STEP ---\n",
    "# Take 10% of data for the heavy tuning process\n",
    "train_vec_sample = train_vec.sample(False, 0.2, seed=42) \n",
    "train_vec_sample.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "print(f\"   Tuning Sample Rows: {train_vec_sample.count():,}\")\n",
    "\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=target_col, seed=42)\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [50, 100]) \\\n",
    "    .build()\n",
    "    # .addGrid(rf.maxDepth, [10, 15]) \\\n",
    "    # .addGrid(rf.maxBins, [32, 64]) \\\n",
    "    \n",
    "\n",
    "\n",
    "print(f\"   Combinations to test: {len(paramGrid)}\")\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=target_col, predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "# tvs = TrainValidationSplit(\n",
    "#     estimator=rf,\n",
    "#     estimatorParamMaps=paramGrid,\n",
    "#     evaluator=evaluator,\n",
    "#     trainRatio=0.8,\n",
    "#     parallelism=1\n",
    "# )\n",
    "\n",
    "\n",
    "# print(\"3. Running Train-Validation Split (Please wait)...\")\n",
    "# start_time = time.time()\n",
    "\n",
    "# cv_model = tvs.fit(train_vec)\n",
    "\n",
    "# end_time = time.time()\n",
    "# duration = end_time - start_time\n",
    "# print(f\"   Tuning completed in {duration/60:.2f} minutes!\")\n",
    "\n",
    "# print(\"4. Analyzing the Champion Model...\")\n",
    "\n",
    "# best_model = cv_model.bestModel\n",
    "\n",
    "tvs = TrainValidationSplit(\n",
    "    estimator=rf,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    trainRatio=0.8,\n",
    "    parallelism=1\n",
    ")\n",
    "\n",
    "print(\"3. Running Train-Validation Split (Please wait)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Pass the SAMPLE to the tuner\n",
    "cv_model = tvs.fit(train_vec_sample) \n",
    "\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "print(f\"   Tuning completed in {duration/60:.2f} minutes!\")\n",
    "\n",
    "print(\"4. Analyzing the Champion Model...\")\n",
    "best_model = cv_model.bestModel\n",
    "\n",
    "best_model.write().overwrite().save(os.path.join(model_output_dir, \"spark_model_winner\"))\n",
    "\n",
    "save_best_params(cv_model, model_output_dir)\n",
    "\n",
    "test_preds = best_model.transform(test_vec)\n",
    "evaluate_and_log(test_preds, target_col, duration, model_output_dir, MODEL_NAME)\n",
    "\n",
    "print(\"\\n--- PROCESS FINISHED ---\")\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5409afd",
   "metadata": {},
   "source": [
    "## Lag Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a86a8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"RF_Lags_Tuned\"\n",
    "Previous_Tuning_Dir = \"optimized_models/RandomForest_HyperTuning\"\n",
    "BASE_OUTPUT_DIR = \"saved_models\"\n",
    "MASTER_LOG_FILE = \"model_comparison.csv\"\n",
    "BASE_DATA_PATH = \"processed_data\"\n",
    "\n",
    "print(f\"--- STARTING THE ULTIMATE MODEL: {MODEL_NAME} ---\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(MODEL_NAME) \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"10g\") \\\n",
    "    .config(\"spark.executor.memory\", \"10g\") \\\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"128m\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "model_output_dir = os.path.join(BASE_OUTPUT_DIR, MODEL_NAME)\n",
    "if os.path.exists(model_output_dir):\n",
    "    shutil.rmtree(model_output_dir)\n",
    "os.makedirs(model_output_dir)\n",
    "\n",
    "def evaluate_and_log(predictions, target_col, time_taken, output_dir, model_name):\n",
    "    \"\"\"Calculates metrics and appends them to the Master CSV.\"\"\"\n",
    "    evaluator = RegressionEvaluator(labelCol=target_col, predictionCol=\"prediction\")\n",
    "    \n",
    "    r2 = evaluator.setMetricName(\"r2\").evaluate(predictions)\n",
    "    rmse = evaluator.setMetricName(\"rmse\").evaluate(predictions)\n",
    "    mae = evaluator.setMetricName(\"mae\").evaluate(predictions)\n",
    "    \n",
    "    snow_subset = predictions.filter(col(target_col) > 0)\n",
    "    r2_snow = evaluator.setMetricName(\"r2\").evaluate(snow_subset) if snow_subset.count() > 0 else 0.0\n",
    "\n",
    "    metrics = {\n",
    "        \"Model\": model_name,\n",
    "        \"Time_Min\": round(time_taken / 60, 2),\n",
    "        \"R2_Global\": round(r2, 4),\n",
    "        \"RMSE_Global\": round(rmse, 4),\n",
    "        \"MAE_Global\": round(mae, 4),\n",
    "        \"R2_Snow_Only\": round(r2_snow, 4)\n",
    "    }\n",
    "\n",
    "    file_exists = os.path.isfile(MASTER_LOG_FILE)\n",
    "    with open(MASTER_LOG_FILE, mode='a', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=metrics.keys())\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(metrics)\n",
    "    \n",
    "    with open(os.path.join(output_dir, \"metrics.json\"), 'w') as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "\n",
    "    print(f\"\\n--- RESULTS ({model_name}) ---\")\n",
    "    print(f\"R2 Global: {r2:.4f} | R2 Snow Only: {r2_snow:.4f}\")\n",
    "    return metrics\n",
    "\n",
    "print(\"1. Loading best parameters from previous Tuning...\")\n",
    "\n",
    "params_file = os.path.join(Previous_Tuning_Dir, \"best_params.json\")\n",
    "\n",
    "try:\n",
    "    with open(params_file, 'r') as f:\n",
    "        best_params = json.load(f)\n",
    "    print(\"   [v] Parameters loaded successfully:\")\n",
    "    print(json.dumps(best_params, indent=4))\n",
    "except FileNotFoundError:\n",
    "    print(f\"   [!] ERROR: {params_file} not found. Make sure the Tuning step finished successfully.\")\n",
    "    spark.stop()\n",
    "    raise\n",
    "\n",
    "\n",
    "train_years = range(2010, 2021) \n",
    "test_years  = range(2023, 2025) \n",
    "\n",
    "def get_paths(years):\n",
    "    return [f\"{BASE_DATA_PATH}/climate_{y}.parquet\" for y in years]\n",
    "\n",
    "print(\"2. Loading data and generating Lags...\")\n",
    "train_raw = spark.read.parquet(*get_paths(train_years))\n",
    "test_raw  = spark.read.parquet(*get_paths(test_years))\n",
    "\n",
    "def add_features_with_lags(df):\n",
    "    # Basic Physics\n",
    "    df = df.withColumn(\"MONTH\", month(col(\"DATE\")))\n",
    "    df = df.withColumn(\"Solid_PRCP\", when((col(\"PRCP\") > 0) & (col(\"TEMP\") < 2.0), col(\"PRCP\")).otherwise(0.0))\n",
    "    \n",
    "    # LAG FEATURES\n",
    "    windowSpec = Window.partitionBy(\"STATION\").orderBy(\"DATE\")\n",
    "    \n",
    "    df = df.withColumn(\"SNDP_lag1\", lag(\"SNDP\", 1).over(windowSpec)) \\\n",
    "           .withColumn(\"TEMP_lag1\", lag(\"TEMP\", 1).over(windowSpec)) \\\n",
    "           .withColumn(\"PRCP_lag1\", lag(\"PRCP\", 1).over(windowSpec))\n",
    "    \n",
    "    return df.na.drop(subset=[\"SNDP_lag1\", \"TEMP_lag1\"])\n",
    "\n",
    "train_df = add_features_with_lags(train_raw)\n",
    "test_df  = add_features_with_lags(test_raw)\n",
    "\n",
    "target_col = \"SNDP\"\n",
    "ignore_cols = [target_col, \"DATE\", \"STATION\", \"NAME\", \"features\", \"prediction\", \"FRSHTT\"]\n",
    "valid_types = ['int', 'bigint', 'float', 'double', 'tinyint', 'smallint']\n",
    "\n",
    "dtypes = train_df.dtypes\n",
    "feature_cols = [c for c, t in dtypes if t in valid_types and c not in ignore_cols]\n",
    "print(f\"   Final Features ({len(feature_cols)}): {feature_cols}\")\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\").setHandleInvalid(\"skip\")\n",
    "\n",
    "train_vec = assembler.transform(train_df)\n",
    "test_vec  = assembler.transform(test_df)\n",
    "\n",
    "train_vec.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "print(f\"   Final Training Rows: {train_vec.count():,}\")\n",
    "\n",
    "print(\"3. Training Ultimate Model (Best Params + Lags)...\")\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=target_col,\n",
    "    seed=42,\n",
    "    numTrees=int(best_params[\"numTrees\"]),\n",
    "    maxDepth=int(best_params[\"maxDepth\"]),\n",
    "    maxBins=int(best_params[\"maxBins\"]),\n",
    "    subsamplingRate=0.7 \n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "model = rf.fit(train_vec)\n",
    "duration = time.time() - start_time\n",
    "\n",
    "print(f\"   Training completed in {duration/60:.2f} minutes.\")\n",
    "\n",
    "print(\"4. Evaluating and Logging final results...\")\n",
    "\n",
    "model.write().overwrite().save(os.path.join(model_output_dir, \"spark_model_ultimate\"))\n",
    "\n",
    "test_preds = model.transform(test_vec)\n",
    "\n",
    "metrics = evaluate_and_log(test_preds, target_col, duration, model_output_dir, MODEL_NAME)\n",
    "\n",
    "print(\"\\n--- PROCESS FINISHED ---\")\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
